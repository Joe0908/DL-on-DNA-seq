1. Motivation of the Project
I want to simulate a biological motif detection task:
Given a 100-bp DNA sequence, detect whether a known transcription-factor motif (CTCF) is present somewhere inside the sequence.
This mirrors real TFBS prediction, but I start with a controlled synthetic dataset so I can test different deep-learning models.

2. Biological Foundation: Using the PWM of CTCF
Instead of implanting a fixed sequence (like ATGCCA), I used a real biological PWM (MA0139.1). Each position has probabilities for A/C/G/T, and a “motif instance” is generated by sampling one base per column.
This makes the positive sequences biologically realistic because:
Every instance of CTCF is slightly different
Motifs follow true biochemical variability
CNN saliency maps will align with meaningful patterns
This mimics how real transcription factors bind.

3. Creating the Dataset
3.1 Generate 4000 sequences
Half are positives (motif implanted), half are negatives (random background).
Each sequence is 100 bp.
3.2 For positive examples
The code:
• samples a realistic motif using PWM
• inserts it at a random location
• keeps everything else a random background
This is exactly how synthetic TFBS benchmarks are usually built in computational biology.
3.3 One-hot encoding
Each nucleotide → 4-channel vector:
A = [1,0,0,0]
C = [0,1,0,0]
G = [0,0,1,0]
T = [0,0,0,1]
The result is a tensor of shape:
(4000, 100, 4).

4. Manual Train/Val/Test Split
I split the data manually instead of sklearn:
70% train
15% validation
15% test
This prevents mixing sequences accidentally and ensures fair comparison across models.

5. Model Architectures
I implemented four classic deep-learning models:
1. CNN (best performer)
Why CNN works best:
Convolution detects local motifs
Max pooling finds the most activated region
Fit biological intuition: TFBS ≈ short local patterns
2. SimpleRNN
Struggles because RNNs are bad with long sequences and noisy backgrounds.
3. LSTM
Should be better than RNN but still not great here because motif detection is not a long-range dependency task.
4. Transformer
Performs decently, but with 1 layer and tiny embedding, it cannot outperform CNN.
Your results reflect real bioinformatics benchmarks:
CNN > Transformer > LSTM > RNN.
6. Training Procedure
For each model:
used BCE loss
trained 15 epochs
tracked validation accuracy & loss
evaluated on the test set
I store the histories for later plotting.
The CNN rapidly reaches 97–98% val accuracy.
Others hover around randomness (~50–60%).

7. Plotting: Model Comparison
I generated two line charts:
Validation Accuracy
Shows CNN dominating from epoch 1 onwards.
Validation Loss
CNN’s loss decreases beautifully.
Others remain flat; they fail to learn the motif reliably.
These plots clearly demonstrate the difference between architectures on motif detection.

8. Ranking the Models
I sort the final test accuracies:
1.	CNN (0.9633)
2.	Transformer (0.63)
3.	LSTM (0.525)
4.	SimpleRNN (~0.48)
This ranking is printed and also visualised.

9. Saliency Map 
This is the most biologically meaningful part.
Choose the best model (CNN) and:
Pick a correct positive example
compute the gradient of the output with respect to the input
Take absolute gradient: importance score
plot as a heatmap across positions
The saliency map lights up exactly on the real motif region
(around position ~55–65 in your example).
This matches ground truth and proves that:
The CNN really learned to detect the CTCF PWM motif.

